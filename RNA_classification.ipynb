{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) MakeNcRNAmatrix\n",
    "\n",
    "    Input : fasta format\n",
    "    Output : ./outdir\n",
    "\n",
    "## 1.2) AssemblyMatrix\n",
    "    \n",
    "    Input : ./outdir\n",
    "    Output : \"ncRNApair_data.npy\" And \"ncRNApair_labe.npy\"\n",
    "\n",
    "## 2) DeepLearning\n",
    "\n",
    "    Input : \"ncRNApair_data.npy\" And \"ncRNApair_labe.npy\"\n",
    "    Output :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.78-cp38-cp38-win32.whl (2.2 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\samsa\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from biopython) (1.18.4)\n",
      "Installing collected packages: biopython\n",
      "Successfully installed biopython-1.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\samsa\\appdata\\local\\programs\\python\\python38-32\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MakeNcRNAmatrix\n",
    "\n",
    "    Input : fasta format\n",
    "    Output : ./outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a7acfc26bfb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSeqIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fasta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0mid_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mid_parts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid_part\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Bio\\SeqIO\\__init__.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(handle, format, alphabet)\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[0miterator_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FormatToIterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mAlignIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FormatToIterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[1;31m# Use Bio.AlignIO to read in the alignments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Bio\\SeqIO\\FastaIO.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, source, alphabet, title2ids)\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The alphabet argument is no longer supported\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle2ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle2ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Fasta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Bio\\SeqIO\\Interfaces.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, source, alphabet, mode, fmt)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The alphabet argument is no longer supported\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_close_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# not a path, assume we received a stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "from Bio import SeqIO\n",
    "import random\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_DAFS(path):\n",
    "    inn = subprocess.Popen(['dafscnn', path], stdout=subprocess.PIPE)\n",
    "    buf = []\n",
    "    while True:\n",
    "        innline = inn.stdout.readline().decode('utf-8')\n",
    "        buf.append(innline)\n",
    "        \n",
    "        if not innline:\n",
    "            break\n",
    "\n",
    "    return buf[4].strip().upper(), buf[6].strip().upper()\n",
    "\n",
    "def get_basepair_matrix(path_to_bpfile):\n",
    "    \n",
    "    mat = []\n",
    "    lineArray =[]\n",
    "    seqlen = []\n",
    "    l = 0\n",
    "    ## Store data in list(mat) from input file.\n",
    "    for line in open(path_to_bpfile,'r'):\n",
    "        if line[0] == '>':\n",
    "            mat = mat + [lineArray]\n",
    "            seqlen = seqlen + [l]\n",
    "            \n",
    "            lineArray = []\n",
    "            l = 0\n",
    "        else:\n",
    "            li = re.split('[\\s]', line)\n",
    "            li.pop()\n",
    "            lineArray = lineArray + [li]\n",
    "            l += 1\n",
    "    mat = mat + [lineArray]\n",
    "    seqlen = seqlen + [l]\n",
    "    mat.pop(0)\n",
    "    seqlen.pop(0)\n",
    "    \n",
    "    i = 0\n",
    "    Array = []\n",
    "    ## Make base-pair probability matrix(Array) from list(mat)\n",
    "    for n in seqlen:\n",
    "        arr = np.zeros((n,n), float)\n",
    "        for j in range(n):\n",
    "            for k in mat[i][j]:\n",
    "                rate = re.split('[:]', k)\n",
    "                if len(rate) > 1:\n",
    "                    arr[j][int(rate[0])] = float(rate[1])\n",
    "        i += 1\n",
    "        Array = Array + [arr]\n",
    "\n",
    "    bp_mat = []\n",
    "    ## Calcurate base-pair probability\n",
    "    for i in range(len(seqlen)):\n",
    "        bp = np.zeros([seqlen[i], 3], dtype=np.float32)\n",
    "        # each base\n",
    "        for j in range(seqlen[i]):\n",
    "            Left = 0\n",
    "            Right = 0\n",
    "            # probability of (\n",
    "            for k in range(j, seqlen[i]):\n",
    "                Left += Array[i][j][k]\n",
    "            # probability of )\n",
    "            for k in range(j):\n",
    "                Right += Array[i][k][j]\n",
    "            bp[j][0] = Left\n",
    "            bp[j][1] = Right\n",
    "            bp[j][2] = 1 - Left - Right\n",
    "            \n",
    "        bp_mat = bp_mat + [bp]\n",
    "    #print(bp_mat)\n",
    "\n",
    "    return bp_mat\n",
    "\n",
    "def make_pairFASTA(dataset, itr1, outpath):\n",
    "    train_data = np.empty((0,1200,16), dtype=np.float32)\n",
    "    train_label = np.empty((0), dtype=np.int32)\n",
    "\n",
    "    for j in range(itr1+1, len(dataset)):\n",
    "        # make pairFASTA file\n",
    "        path_to_pairFasta = \"./pair\" + str(itr1) + \",\" + str(j) + \".fa\"\n",
    "        f = open(path_to_pairFasta, 'w')\n",
    "        pairFa = \"\"\n",
    "        for k in [dataset[itr1], dataset[j]]:\n",
    "            pairFa += \">\" + k[0] + \"\\n\" + k[2] + \"\\n\"\n",
    "        f.write(pairFa)\n",
    "        f.close()\n",
    "\n",
    "        # calculate pairwise alignment score by DAFS\n",
    "        pair1, pair2 = run_DAFS(path_to_pairFasta)\n",
    "        subprocess.call([\"rm\", path_to_pairFasta])\n",
    "\n",
    "        # get base pair matrix\n",
    "        path_to_bpfile = \"bp\" + dataset[itr1][0] + \"_\" + dataset[j][0] + \".txt\"\n",
    "        bp_mat = get_basepair_matrix(path_to_bpfile)\n",
    "        subprocess.call([\"rm\", path_to_bpfile])\n",
    "\n",
    "        # make train data\n",
    "        data = np.zeros(([1200,16]), dtype=np.float32)\n",
    "        n1 = 0 \n",
    "        n2 = 0 \n",
    "        for k in range(len(pair1)):\n",
    "            ## pair1's data\n",
    "             # bases data\n",
    "            if pair1[k] == \"A\":\n",
    "                data[k][0] = 1 \n",
    "            elif pair1[k] == \"T\":\n",
    "                data[k][1] = 1 \n",
    "            elif pair1[k] == \"G\":\n",
    "                data[k][2] = 1 \n",
    "            elif pair1[k] == \"C\":\n",
    "                data[k][3] = 1 \n",
    "            elif pair1[k] == \"-\":\n",
    "                data[k][4] = 1 \n",
    "             # base-pair probability data\n",
    "            if pair1[k] != \"-\":\n",
    "                data[k][5] = bp_mat[0][n1][0]\n",
    "                data[k][6] = bp_mat[0][n1][1]\n",
    "                data[k][7] = bp_mat[0][n1][2]\n",
    "                n1 += 1\n",
    "            \n",
    "            ## pair2's data\n",
    "             # bases data\n",
    "            if pair2[k] == \"A\":\n",
    "                data[k][8] = 1 \n",
    "            elif pair2[k] == \"T\":\n",
    "                data[k][9] = 1 \n",
    "            elif pair2[k] == \"G\":\n",
    "                data[k][10] = 1 \n",
    "            elif pair2[k] == \"C\":\n",
    "                data[k][11] = 1 \n",
    "            elif pair2[k] == \"-\":\n",
    "                data[k][12] = 1 \n",
    "            # base-pair probability data\n",
    "            if pair2[k] != \"-\":\n",
    "                data[k][13] = bp_mat[1][n2][0]\n",
    "                data[k][14] = bp_mat[1][n2][1]\n",
    "                data[k][15] = bp_mat[1][n2][2]\n",
    "                n2 += 1\n",
    "\n",
    "        # make train label\n",
    "        if dataset[itr1][1] == dataset[j][1]:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "\n",
    "        train_data = np.append(train_data, np.array([data]), axis=0)\n",
    "        train_label = np.append(train_label, label)\n",
    "            \n",
    "    #print(train_data)\n",
    "    #print(train_label)\n",
    "\n",
    "    outdata = outpath + \"/portion/ncRNApair_data\" + str(itr1) + \".npy\"\n",
    "    outlabel = outpath + \"/portion/ncRNApair_label\" + str(itr1) + \".npy\"    \n",
    "\n",
    "    np.save(outdata, train_data)\n",
    "    np.save(outlabel, train_label)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    infile = sys.argv[1]\n",
    "    itr1 = 1\n",
    "    outpath = './outdir'\n",
    "\n",
    "    if outpath[-1] == \"/\":\n",
    "        outpath = outpath[:-1]\n",
    "\n",
    "    ###### read fasta file #######\n",
    "    dataset = []\n",
    "    out = \"\"\n",
    "    for record in SeqIO.parse(infile, \"fasta\"):\n",
    "        id_part = record.id\n",
    "        id_parts = id_part.split(\",\")\n",
    "        seq_part = str(record.seq.upper())\n",
    "\n",
    "        # geneset : [[gene name, genelabel(mi=0,sno=1,t=2), sequence],...\n",
    "        dataset = dataset + [[id_parts[0], int(id_parts[1]), seq_part]]\n",
    "        \n",
    "        out += id_parts[0] + \":\" + id_parts[1] + \"\\n\"\n",
    "    f = open(outpath+\"/genelabel.txt\", \"w\")\n",
    "    f.write(out)\n",
    "    f.close()\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    make_pairFASTA(dataset, itr1, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AssemblyMatrix\n",
    "    Input : ./outdir\n",
    "    Output : \"ncRNApair_data.npy\" And \"ncRNApair_labe.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembledata(ddir):\n",
    "\n",
    "    checkdata = np.load(ddir+\"/portion/ncRNApair_data0.npy\")\n",
    "    dsize = len(checkdata) + 1\n",
    "    print('dsize =',dsize)\n",
    "    genelen = len(checkdata[0])\n",
    "    width = len(checkdata[0][0])\n",
    "\n",
    "    data = np.empty((0,genelen,width), dtype=np.float32)\n",
    "    label = np.empty((0), dtype=np.int32)\n",
    "\n",
    "    print(\"Makeing data...\")\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(2):\n",
    "        indata = ddir + \"/portion/ncRNApair_data\"+str(i)+\".npy\"\n",
    "        inlabel = ddir + \"/portion/ncRNApair_label\"+str(i)+\".npy\"\n",
    "\n",
    "        tmpdata = np.load(indata)\n",
    "        tmplabel = np.load(inlabel)\n",
    "        \n",
    "        print(tmpdata.shape[0])\n",
    "\n",
    "        data = np.concatenate([data, tmpdata], axis=0)\n",
    "        label = np.concatenate([label, tmplabel], axis=0)\n",
    "\n",
    "    print(\"Complete makeing data.\")\n",
    "    elapsed_time = time.time() - start\n",
    "    print (\"Loding time:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "    print(\"Data size :\",len(data),\"*\",len(data[0]),\"*\",len(data[0][0]))\n",
    "\n",
    "    print('here :',ddir+\"/ncRNApair_data.npy\")\n",
    "    np.save(\"ncRNApair_data.npy\", data)\n",
    "    np.save(\"ncRNApair_label.npy\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temo =  np.load(\"ncRNApair_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsize = 60\n",
      "Makeing data...\n",
      "59\n",
      "58\n",
      "Complete makeing data.\n",
      "Loding time:0.018949508666992188[sec]\n",
      "Data size : 117 * 1200 * 16\n",
      "here : ./outdir/ncRNApair_data.npy\n"
     ]
    }
   ],
   "source": [
    "datadir = sys.argv[1]\n",
    "if datadir[-1] == \"/\":\n",
    "    datadir = datadir[:-1]\n",
    "\n",
    "assembledata('./outdir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "genelabellist = []\n",
    "for line in open('./outdir/genelabel.txt', 'r'):\n",
    "    lines = line.strip().split(\":\")    # Separater is \":\".\n",
    "    genelabellist.append(int(lines[1]))\n",
    "    \n",
    "genelabellist = np.asarray(genelabellist)\n",
    "inputfam = np.unique(genelabellist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLearning\n",
    "    Input : \"ncRNApair_data.npy\" And \"ncRNApair_labe.npy\"\n",
    "    Output : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load('ncRNApair_data.npy')\n",
    "y_test = np.load('ncRNApair_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('testdata/ncRNApairdataDAFS_test.npy')\n",
    "y_train = np.load('testdata/ncRNApairlabel_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (1770, 1200, 16)\n",
      "x_test.shape = (117, 1200, 16)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape =',x_train.shape)\n",
    "print('x_test.shape =',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=(1200,16)))\n",
    "cnn.add(tf.keras.layers.MaxPool1D(pool_size=2, strides=2))\n",
    "cnn.add(tf.keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool1D(pool_size=2, strides=2))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "cnn.add(tf.keras.layers.BatchNormalization())\n",
    "cnn.add(tf.keras.layers.Dropout(.4))\n",
    "cnn.add(tf.keras.layers.Dense(units=8, activation='relu'))\n",
    "cnn.add(tf.keras.layers.BatchNormalization())\n",
    "cnn.add(tf.keras.layers.Dropout(.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.7593 - accuracy: 0.5463\n",
      "Epoch 2/5\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.6428 - accuracy: 0.6373\n",
      "Epoch 3/5\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.5892 - accuracy: 0.6927\n",
      "Epoch 4/5\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 0.5077 - accuracy: 0.7814\n",
      "Epoch 5/5\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 0.4582 - accuracy: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cd51444fc8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = x_train, y = y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for predicting training set : 0.867231638418079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def get_res(y_pred):\n",
    "    res_test = []\n",
    "    for i in y_pred:\n",
    "        if i<0.5: res_test.append(0)\n",
    "        else:\n",
    "            res_test.append(1)\n",
    "    return res_test\n",
    "\n",
    "y_pred_train = cnn.predict(x_train)\n",
    "y_pred_train = y_pred_train.reshape((1770,))\n",
    "\n",
    "res = get_res(y_pred_train)\n",
    "confusion_matrix(y_train, res)\n",
    "print('Accuracy for predicting training set :',accuracy_score(res,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for predicting test set : 0.8632478632478633\n"
     ]
    }
   ],
   "source": [
    "y_test_pred  = cnn.predict(x_test)\n",
    "y_test_pred = y_test_pred.reshape((117,))\n",
    "res = get_res(y_test_pred)\n",
    "confusion_matrix(y_test, res)\n",
    "print('Accuracy for predicting test set :',accuracy_score(res,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
